"""
å¢å¼ºå‹æ¨¡å‹è¯„ä¼°æ¨¡å—
åŠŸèƒ½ï¼šå¤šç›®æ ‡è¯„ä¼°ã€åŠ¨æ€æŒ‡æ ‡ç®¡ç†ã€äº¤äº’å¼å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ
ç‰ˆæœ¬ï¼š2.1
"""

import json
from pathlib import Path
from typing import Dict, List, Optional, Union

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from joblib import load
from scipy.stats import spearmanr
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tqdm.auto import tqdm


class EnhancedEvaluator:
    def __init__(
            self,
            target_names: List[str] = ["SPAD", "N"],
            metrics_config: Optional[Dict[str, Dict]] = None,
            output_dir: str = "reports",
            progress_theme: str = "classic"
            ):
        """
        åˆå§‹åŒ–å¢å¼ºå‹è¯„ä¼°å™¨

        :param target_names: ç›®æ ‡å˜é‡åç§°åˆ—è¡¨ (æ”¯æŒå¤šç›®æ ‡)
        :param metrics_config: æŒ‡æ ‡é…ç½®å­—å…¸ {æŒ‡æ ‡å: {func: è®¡ç®—å‡½æ•°, kwargs: å‚æ•°å­—å…¸}}
        :param output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        :param progress_theme: è¿›åº¦æ¡ä¸»é¢˜ ('classic'/'modern')
        """
        self.target_names = target_names
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.progress_theme = progress_theme

        # åˆå§‹åŒ–é»˜è®¤æŒ‡æ ‡é…ç½®
        self.metrics = self._init_metrics(metrics_config)

        # é…ç½®å¯è§†åŒ–é£æ ¼
        sns.set(style="whitegrid", palette="muted")
        plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['axes.unicode_minus'] = False

    def _init_metrics(self, config: Optional[Dict]) -> Dict:
        """åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡"""
        default_metrics = {
            'R2': {
                'func': r2_score,
                'kwargs': {},
                'format': '.3f'
                },
            'RMSE': {
                'func': lambda y, p: np.sqrt(mean_squared_error(y, p)),
                'kwargs': {},
                'format': '.2f'
                },
            'MAE': {
                'func': mean_absolute_error,
                'kwargs': {},
                'format': '.2f'
                },
            'Spearman': {
                'func': lambda y, p: spearmanr(y, p).correlation,
                'kwargs': {},
                'format': '.3f'
                },
            'Tolerance10%': {
                'func': lambda y, p: np.mean(np.abs((y - p) / y) <= 0.1) * 100,
                'kwargs': {},
                'format': '.1f%'
                }
            }
        return config or default_metrics

    def evaluate(
            self,
            model_paths: List[Union[str, Path]],
            X_test: np.ndarray,
            y_test: np.ndarray,
            model_types: List[str],
            save_details: bool = True
            ) -> pd.DataFrame:
        """
        æ‰§è¡Œå¤šæ¨¡å‹å¤šç›®æ ‡è¯„ä¼°

        :param model_paths: æ¨¡å‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        :param X_test: æµ‹è¯•é›†ç‰¹å¾çŸ©é˜µ
        :param y_test: æµ‹è¯•é›†ç›®æ ‡å€¼ (n_samples, n_targets)
        :param model_types: æ¨¡å‹ç±»å‹åˆ—è¡¨ ('sklearn'/'keras')
        :param save_details: æ˜¯å¦ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
        :return: è¯„ä¼°ç»“æœDataFrame
        """
        results = []
        pbar_config = self._get_progress_config()

        with tqdm(total=len(model_paths), **pbar_config) as main_pbar:
            for path, mtype in zip(model_paths, model_types):
                main_pbar.set_description(f"è¯„ä¼° {Path(path).stem}")

                # åŠ è½½æ¨¡å‹
                model = self._load_model(path, mtype)
                if model is None:
                    continue

                # æ‰§è¡Œé¢„æµ‹
                y_pred = self._predict_with_progress(model, X_test, mtype)

                # è®¡ç®—å¤šç›®æ ‡æŒ‡æ ‡
                model_metrics = {}
                for target_idx, target in enumerate(self.target_names):
                    y_true_single = y_test[:, target_idx]
                    y_pred_single = y_pred[:, target_idx]

                    for metric, config in self.metrics.items():
                        key = f"{target}_{metric}"
                        try:
                            value = config['func'](y_true_single, y_pred_single, **config['kwargs'])
                            model_metrics[key] = float(value)
                        except Exception as e:
                            print(f"è®¡ç®— {metric} å¤±è´¥: {str(e)}")
                            model_metrics[key] = np.nan

                # è®°å½•ç»“æœ
                model_metrics["Model"] = Path(path).stem
                results.append(model_metrics)

                # ä¿å­˜è¯¦ç»†ä¿¡æ¯
                if save_details:
                    self._save_prediction_details(
                            y_test, y_pred, path, target_names=self.target_names
                            )

                main_pbar.update(1)

        # ç”ŸæˆæŠ¥å‘Š
        report_df = pd.DataFrame(results)
        self._save_reports(report_df, y_test)
        return report_df

    def _load_model(self, path: Union[str, Path], model_type: str):
        """å®‰å…¨åŠ è½½æ¨¡å‹"""
        try:
            if model_type == "keras":
                from keras.models import load_model
                return load_model(path)
            return load(path)
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥ {Path(path).name}: {str(e)}")
            return None

    def _predict_with_progress(self, model, X: np.ndarray, model_type: str) -> np.ndarray:
        """å¸¦å†…å­˜ç®¡ç†çš„é¢„æµ‹æµç¨‹"""
        # è°ƒæ•´è¾“å…¥ç»´åº¦
        if model_type == "keras":
            X = X.reshape(X.shape[0], -1, 1)

        # åˆ†å—é¢„æµ‹é…ç½®
        chunk_size = min(512, len(X))
        preds = []
        pbar_config = self._get_progress_config(desc="é¢„æµ‹è¿›åº¦")

        with tqdm(total=len(X), **pbar_config) as pred_pbar:
            for i in range(0, len(X), chunk_size):
                chunk = X[i:i + chunk_size]

                try:
                    if model_type == "keras":
                        pred = model.predict(chunk, verbose=0)
                    else:
                        pred = model.predict(chunk)
                    preds.append(pred)
                except Exception as e:
                    print(f"é¢„æµ‹å¤±è´¥: {str(e)}")
                    preds.append(np.full((len(chunk), len(self.target_names)), np.nan))

                pred_pbar.update(len(chunk))

        return np.vstack(preds)

    def _save_prediction_details(
            self, y_true: np.ndarray, y_pred: np.ndarray,
            model_path: Path, target_names: List[str]
            ):
        """ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ"""
        details = {}
        for idx, name in enumerate(target_names):
            details.update(
                    {
                        f"True_{name}": y_true[:, idx],
                        f"Pred_{name}": y_pred[:, idx],
                        f"AE_{name}": np.abs(y_true[:, idx] - y_pred[:, idx]),
                        f"RE_{name}": np.abs((y_true[:, idx] - y_pred[:, idx]) / (y_true[:, idx] + 1e-6))
                        }
                    )

        df = pd.DataFrame(details)
        save_path = self.output_dir / f"details_{model_path.stem}.parquet"
        df.to_parquet(save_path)  # ä½¿ç”¨Parquetæ ¼å¼èŠ‚çœç©ºé—´

    def _save_reports(self, df: pd.DataFrame, y_test: np.ndarray):
        """ç”Ÿæˆå¹¶ä¿å­˜æ‰€æœ‰æŠ¥å‘Š"""
        # CSVæŠ¥å‘Š
        csv_path = self.output_dir / "performance_report.csv"
        df.to_csv(csv_path, index=False)

        # JSONæ‘˜è¦
        summary = {
            "best_models": {},
            "avg_metrics": {}
            }
        for target in self.target_names:
            target_metrics = [m for m in df.columns if m.startswith(f"{target}_")]
            best_model = df.loc[df[f"{target}_R2"].idxmax()]
            summary["best_models"][target] = {
                "name": best_model["Model"],
                "R2": best_model[f"{target}_R2"],
                "RMSE": best_model[f"{target}_RMSE"]
                }

        with open(self.output_dir / "summary.json", 'w') as f:
            json.dump(summary, f, indent=2)

        # å¯è§†åŒ–æŠ¥å‘Š
        self._generate_visualizations(df, y_test)

    def _generate_visualizations(self, df: pd.DataFrame, y_test: np.ndarray):
        """ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–æŠ¥å‘Š"""
        plt.figure(figsize=(18, 10))

        # æ¨¡å‹æ€§èƒ½çƒ­åŠ›å›¾
        plt.subplot(2, 2, 1)
        metrics_to_plot = [m for m in self.metrics if m != 'Tolerance10%']
        plot_data = df[[f"{t}_{m}" for t in self.target_names for m in metrics_to_plot]]
        sns.heatmap(plot_data.corr(), annot=True, cmap="coolwarm")
        plt.title("æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾")

        # å¤šç›®æ ‡R2å¯¹æ¯”
        plt.subplot(2, 2, 2)
        for target in self.target_names:
            sns.kdeplot(df[f"{target}_R2"], label=target, fill=True)
        plt.xlabel("RÂ² Score")
        plt.legend()
        plt.title("å„ç›®æ ‡RÂ²åˆ†å¸ƒ")

        # è¯¯å·®åˆ†å¸ƒé›·è¾¾å›¾
        plt.subplot(2, 2, 3, polar=True)
        metrics = ['RMSE', 'MAE', 'Spearman', 'Tolerance10%']
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        for idx, model in enumerate(df["Model"]):
            values = []
            for metric in metrics:
                values.append(np.mean([df.loc[idx, f"{t}_{metric}"] for t in self.target_names]))
            values += values[:1]
            plt.plot(angles + angles[:1], values, label=model, alpha=0.5)
        plt.xticks(angles, metrics)
        plt.title("å¤šç›®æ ‡è¯¯å·®é›·è¾¾å›¾")

        # çœŸå®å€¼-é¢„æµ‹å€¼åˆ†å¸ƒ
        plt.subplot(2, 2, 4)
        for target in self.target_names:
            true_values = y_test[:, self.target_names.index(target)]
            plt.scatter(true_values, true_values, alpha=0.3, label=f"{target} åŸºå‡†çº¿")
            for model in df["Model"]:
                preds = pd.read_parquet(self.output_dir / f"details_{model}.parquet")[f"Pred_{target}"]
                plt.scatter(true_values, preds, alpha=0.2, label=model)
        plt.xlabel("çœŸå®å€¼")
        plt.ylabel("é¢„æµ‹å€¼")
        plt.title("é¢„æµ‹å€¼åˆ†å¸ƒå¯¹æ¯”")

        plt.tight_layout()
        plt.savefig(self.output_dir / "visual_summary.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _get_progress_config(self, desc: str = "è¯„ä¼°è¿›åº¦") -> Dict:
        """è·å–è¿›åº¦æ¡æ ·å¼é…ç½®"""
        themes = {
            'classic': {
                'desc': desc,
                'bar_format': "{l_bar}{bar}| {n_fmt}/{total_fmt}",
                'colour': 'green'
                },
            'modern': {
                'desc': f"ğŸ“Š {desc}",
                'bar_format': "{desc}: {percentage:.0f}%|{bar:20}| {n_fmt}/{total_fmt}",
                'colour': '#FF6F00'
                }
            }
        return themes.get(self.progress_theme, themes['classic'])

    @staticmethod
    def create_custom_metric(
            name: str,
            calculation_func: callable,
            format_str: str = ".2f",
            **kwargs
            ) -> Dict:
        """
        åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

        :param name: æŒ‡æ ‡åç§°
        :param calculation_func: è®¡ç®—å‡½æ•° (y_true, y_pred) -> float
        :param format_str: ç»“æœæ ¼å¼åŒ–å­—ç¬¦ä¸²
        :param kwargs: ä¼ é€’ç»™è®¡ç®—å‡½æ•°çš„å›ºå®šå‚æ•°
        :return: æŒ‡æ ‡é…ç½®å­—å…¸
        """
        return {
            name: {
                'func': calculation_func,
                'kwargs': kwargs,
                'format': format_str
                }
            }
